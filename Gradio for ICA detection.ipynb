{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16778,"status":"ok","timestamp":1732241105013,"user":{"displayName":"jidapa jit","userId":"08721025905940533370"},"user_tz":-420},"id":"j-_DQ41nfrZy","outputId":"c34f3a83-2d78-4688-8aa4-d2d79193e42d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 17059, done.\u001b[K\n","remote: Counting objects: 100% (37/37), done.\u001b[K\n","remote: Compressing objects: 100% (25/25), done.\u001b[K\n","remote: Total 17059 (delta 18), reused 27 (delta 12), pack-reused 17022 (from 1)\u001b[K\n","Receiving objects: 100% (17059/17059), 15.68 MiB | 9.65 MiB/s, done.\n","Resolving deltas: 100% (11713/11713), done.\n","/content/yolov5\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd /content/yolov5\n","%pip install -qr requirements.txt  # install dependencies"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7672,"status":"ok","timestamp":1732241112682,"user":{"displayName":"jidapa jit","userId":"08721025905940533370"},"user_tz":-420},"id":"q28yD6bngZ8m","outputId":"212ac9a0-7d91-40f1-8367-03256674427c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 2.5.1+cu121 CPU\n"]}],"source":["import torch\n","from IPython.display import Image  # to display images\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"m_Agrnc7Yfbg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732241365308,"user_tz":-420,"elapsed":16691,"user":{"displayName":"jidapa jit","userId":"08721025905940533370"}},"outputId":"fabd4343-d754-425a-fb04-f8cec60c26d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-5.6.0-py3-none-any.whl.metadata (16 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Collecting fastapi<1.0,>=0.115.2 (from gradio)\n","  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.4.3 (from gradio)\n","  Downloading gradio_client-1.4.3-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n","Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Collecting markupsafe~=2.0 (from gradio)\n","  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart==0.0.12 (from gradio)\n","  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.7.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n","  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting starlette<1.0,>=0.40.0 (from gradio)\n","  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.3->gradio) (2024.10.0)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.3->gradio)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-5.6.0-py3-none-any.whl (57.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.4.3-py3-none-any.whl (320 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n","Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Downloading ruff-0.7.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n","Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","Successfully installed aiofiles-23.2.1 fastapi-0.115.5 ffmpy-0.4.0 gradio-5.6.0 gradio-client-1.4.3 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.4 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.12.0 uvicorn-0.32.1 websockets-12.0\n"]}],"source":["!pip install gradio\n"]},{"cell_type":"markdown","metadata":{"id":"x4Y0IO8PYi_p"},"source":["2. Define the Gradio Interface\n","In the code below, we define a Gradio interface that takes an image input, runs YOLOv5 on it, and returns the output image with bounding boxes."]},{"cell_type":"markdown","metadata":{"id":"wTp3xIS4YkeH"},"source":["#YOLOv5 Inference with Gradio by image"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3Y_1hnRqYdDu","colab":{"base_uri":"https://localhost:8080/","height":835},"executionInfo":{"status":"ok","timestamp":1732241432835,"user_tz":-420,"elapsed":19557,"user":{"displayName":"jidapa jit","userId":"08721025905940533370"}},"outputId":"5b43c030-1217-4999-bfd8-98ad2b6dd673"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  warnings.warn(\n","Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n"]},{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]},{"output_type":"stream","name":"stderr","text":["YOLOv5 🚀 2024-11-22 Python-3.10.12 torch-2.5.1+cu121 CPU\n","\n","Fusing layers... \n","YOLOv5s summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","Adding AutoShape... \n"]},{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://187b61383cabe5bf8d.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://187b61383cabe5bf8d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":4}],"source":["import torch\n","import gradio as gr\n","from PIL import Image\n","import cv2\n","import numpy as np\n","\n","# Load the trained YOLOv5 model\n","model = torch.hub.load('ultralytics/yolov5', 'custom', path='models/best.pt')  # Replace with your model path\n","\n","# Define a function for inference\n","def detect_objects(image):\n","    # Convert Gradio's image to a format suitable for YOLOv5\n","    input_image = Image.fromarray(image)\n","\n","    # Run inference\n","    results = model(input_image)\n","\n","    # Render the results on the original image\n","    results.render()  # Render adds boxes and labels to the image\n","\n","    # Convert to numpy for displaying in Gradio\n","    output_image = results.ims[0]\n","\n","    return output_image\n","\n","# Set up the Gradio interface\n","iface = gr.Interface(\n","    fn=detect_objects,\n","    inputs=gr.Image(type=\"numpy\"),  # Input as numpy array\n","    outputs=\"image\",  # Output as an image with bounding boxes\n","    title=\"YOLOv5 Object Detection\",\n","    description=\"Upload an image and the YOLOv5 model will detect objects.\"\n",")\n","\n","# Launch the Gradio app\n","iface.launch()\n"]},{"cell_type":"markdown","source":["#YOLOv5 Inference with Gradio by VDO.mp4"],"metadata":{"id":"iivOkiDp1V0v"}},{"cell_type":"code","source":["import torch\n","import gradio as gr\n","import cv2\n","import numpy as np\n","import tempfile\n","import os\n","from PIL import Image\n","\n","# Load the trained YOLOv5 model\n","model = torch.hub.load('ultralytics/yolov5', 'custom', path='models/best.pt')  # Replace with your model path\n","\n","# Define a function for inference on video\n","def detect_objects_in_video(video_path):\n","    # Read video file\n","    cap = cv2.VideoCapture(video_path)\n","\n","    # Get video properties\n","    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","    # Temporary path to save output video\n","    output_path = os.path.join(tempfile.gettempdir(), \"detected_output.mp4\")\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n","\n","    # Process each frame\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Convert the frame to a format suitable for YOLOv5\n","        input_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","\n","        # Run inference on the frame\n","        results = model(input_image)\n","\n","        # Render the results on the frame\n","        results.render()  # Adds bounding boxes and labels to the image\n","\n","        # Convert rendered frame back to OpenCV format (BGR)\n","        output_frame = np.array(results.ims[0])\n","        output_frame = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n","\n","        # Write the frame to the output video\n","        out.write(output_frame)\n","\n","    # Release resources\n","    cap.release()\n","    out.release()\n","\n","    return output_path\n","\n","# Set up the Gradio interface\n","iface = gr.Interface(\n","    fn=detect_objects_in_video,\n","    inputs=gr.Video(),  # Input as a video\n","    outputs=gr.Video(),  # Output as a video\n","    title=\"YOLOv5 Object Detection on Video\",\n","    description=\"Upload a video (e.g., vdo.mp4) and the YOLOv5 model will detect objects frame by frame.\"\n",")\n","\n","# Launch the Gradio app\n","iface.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":729},"id":"4RbIR6WWyPa6","executionInfo":{"status":"ok","timestamp":1732241567355,"user_tz":-420,"elapsed":2115,"user":{"displayName":"jidapa jit","userId":"08721025905940533370"}},"outputId":"0b78a904-3e28-42b9-c011-71c811dd1a92"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n","YOLOv5 🚀 2024-11-22 Python-3.10.12 torch-2.5.1+cu121 CPU\n","\n","Fusing layers... \n","YOLOv5s summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","Adding AutoShape... \n"]},{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://d819025561dca9a0fa.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://d819025561dca9a0fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import torch\n","import gradio as gr\n","import numpy as np\n","from PIL import Image\n","\n","# Load the trained YOLOv5 model from best.pt\n","model = torch.hub.load('ultralytics/yolov5', 'custom', path='best.pt')  # Ensure this path is correct\n","\n","# Define a function for real-time inference on video frames\n","def detect_objects(frame):\n","    # Convert frame (NumPy array) to PIL format for YOLOv5\n","    input_image = Image.fromarray(frame)\n","\n","    # Perform inference\n","    results = model(input_image)\n","\n","    # Render the results on the image\n","    results.render()  # Adds bounding boxes and labels to the image\n","\n","    # Convert back to NumPy for display in Gradio\n","    output_image = np.array(results.ims[0])\n","\n","    return output_image\n","\n","# Set up the Gradio interface with live video feed\n","iface = gr.Interface(\n","    fn=detect_objects,\n","    inputs=gr.Video(),  # Webcam input without additional parameters\n","    outputs=\"image\",  # Display output as an image with bounding boxes\n","    live=True,  # Enables real-time processing\n","    title=\"Real-Time YOLOv5 Object Detection\",\n","    description=\"Real-time object detection with YOLOv5 using your camera. Point your camera at objects to detect.\"\n",")\n","\n","# Launch the Gradio app\n","iface.launch(share=True)  # share=True provides a public URL to access on your mobile device\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":694},"id":"nFMMStK22-M4","executionInfo":{"status":"ok","timestamp":1732228516853,"user_tz":-420,"elapsed":2170,"user":{"displayName":"jidapa jit","userId":"08721025905940533370"}},"outputId":"2cd858fd-97ae-4c4d-f390-5f3756d20b67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n","YOLOv5 🚀 2024-11-21 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (NVIDIA L4, 22700MiB)\n","\n","Fusing layers... \n","YOLOv5s summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","Adding AutoShape... \n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://c6c93f024852ad3929.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://c6c93f024852ad3929.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":[],"metadata":{"id":"-ZDqM8UY1QgV"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}